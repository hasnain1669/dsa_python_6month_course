{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Scores:\n",
      "Decision Tree: 0.942\n",
      "Random Forest: 0.942\n",
      "Logistic Regression: 0.967\n",
      "KNN: 0.942\n",
      "SVM: 0.950\n",
      "Naive Bayes: 0.942\n",
      "Gradient Boosting: 0.942\n",
      "\n",
      "Best Model: Logistic Regression\n",
      "Test Set Accuracy: 1.000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the models\n",
    "models = {\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'SVM': SVC(),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'Gradient Boosting': GradientBoostingClassifier()\n",
    "}\n",
    "\n",
    "# Evaluate the models using cross-validation\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    results[name] = cv_scores.mean()\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print('Cross-Validation Scores:')\n",
    "for name, score in results.items():\n",
    "    print(f'{name}: {score:.3f}')\n",
    "\n",
    "# Select the best model\n",
    "best_model_name = max(results, key=results.get)\n",
    "best_model = models[best_model_name]\n",
    "print(f'\\nBest Model: {best_model_name}')\n",
    "\n",
    "# Train the best model on the entire training set\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "test_score = best_model.score(X_test, y_test)\n",
    "print(f'Test Set Accuracy: {test_score:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression CV accuracy: 0.950 +/- 0.085\n",
      "Random Forest CV accuracy: 0.933 +/- 0.104\n",
      "KNN CV accuracy: 0.942 +/- 0.065\n",
      "SVM CV accuracy: 0.950 +/- 0.067\n",
      "Decision Tree CV accuracy: 0.925 +/- 0.102\n",
      "Gradient Boosting CV accuracy: 0.925 +/- 0.102\n",
      "Naive Bayes CV accuracy: 0.942 +/- 0.075\n"
     ]
    }
   ],
   "source": [
    "# import the libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# define the models to be evaluated\n",
    "models = [LogisticRegression(), RandomForestClassifier(), KNeighborsClassifier(), SVC(), DecisionTreeClassifier(), GradientBoostingClassifier(), GaussianNB()]\n",
    "names = ['Logistic Regression', 'Random Forest', 'KNN', 'SVM', 'Decision Tree', 'Gradient Boosting', 'Naive Bayes']\n",
    "\n",
    "# perform k-fold cross-validation for each model\n",
    "k =10\n",
    "for name, model in zip(names, models):\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=k, scoring='accuracy')\n",
    "    print(f'{name} CV accuracy: {np.mean(cv_scores):.3f} +/- {np.std(cv_scores):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': -1, 'probability': False, 'random_state': None, 'shrinking': True, 'tol': 0.001, 'verbose': False}\n"
     ]
    }
   ],
   "source": [
    "SVC = SVC()\n",
    "\n",
    "# get the parameters of SVC\n",
    "print(SVC.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'C': 0.1, 'gamma': 0.1, 'kernel': 'poly'}\n",
      "Best cross-validation score: 0.958\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# define the hyperparameters and their values\n",
    "param_grid = {\n",
    "    'C' : [0.1, 1, 10, 100],\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(SVC(), param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f'Best parameters: {grid_search.best_params_}')\n",
    "print(f'Best cross-validation score: {grid_search.best_score_:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "Random_Forest = RandomForestClassifier()\n",
    "\n",
    "# get the parameters of Random Forest\n",
    "print(Random_Forest.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'max_depth': 5, 'n_estimators': 300}\n",
      "Best cross-validation score: 0.958\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# define the hyperparameters and their values\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 400],\n",
    "    'max_depth': [5, 10, 15, 20],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f'Best parameters: {grid_search.best_params_}')\n",
    "print(f'Best cross-validation score: {grid_search.best_score_:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "Logistic_Reg = LogisticRegression()\n",
    "\n",
    "# get the parameters of Logistic Regression\n",
    "print(Logistic_Reg.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'C': 1, 'solver': 'sag'}\n",
      "Best cross-validation score: 0.975\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# define the hyperparameters and their values\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(LogisticRegression(), param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f'Best parameters: {grid_search.best_params_}')\n",
    "print(f'Best cross-validation score: {grid_search.best_score_:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': None, 'n_neighbors': 5, 'p': 2, 'weights': 'uniform'}\n"
     ]
    }
   ],
   "source": [
    "KNN = KNeighborsClassifier()\n",
    "\n",
    "# get the parameters of KNN\n",
    "print(KNN.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'n_neighbors': 3, 'p': 1, 'weights': 'uniform'}\n",
      "Best cross-validation score: 0.958\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# define the hyperparameters and their values\n",
    "param_grid = {\n",
    "    'n_neighbors': [3, 5, 7, 9, 11],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'p': [1, 2],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f'Best parameters: {grid_search.best_params_}')\n",
    "print(f'Best cross-validation score: {grid_search.best_score_:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'random_state': None, 'splitter': 'best'}\n"
     ]
    }
   ],
   "source": [
    "Decision_Tree = DecisionTreeClassifier()\n",
    "\n",
    "# get the parameters of Decision Tree\n",
    "print(Decision_Tree.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'max_depth': 7, 'min_samples_split': 2}\n",
      "Best cross-validation score: 0.950\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# define the hyperparameters and their values\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7, 9],\n",
    "    'min_samples_split': [2, 4, 6, 8],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f'Best parameters: {grid_search.best_params_}')\n",
    "print(f'Best cross-validation score: {grid_search.best_score_:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.1, 'loss': 'log_loss', 'max_depth': 3, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_iter_no_change': None, 'random_state': None, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "Gradient_Boost = GradientBoostingClassifier()\n",
    "\n",
    "# get the parameters of Gradient Boosting\n",
    "print(Gradient_Boost.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'max_depth': 7, 'n_estimators': 100}\n",
      "Best cross-validation score: 0.950\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# define the hyperparameters and their values\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 400],\n",
    "    'max_depth': [3, 5, 7, 9],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(GradientBoostingClassifier(), param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f'Best parameters: {grid_search.best_params_}')\n",
    "print(f'Best cross-validation score: {grid_search.best_score_:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'priors': None, 'var_smoothing': 1e-09}\n"
     ]
    }
   ],
   "source": [
    "Gaussian_NB = GaussianNB()\n",
    "\n",
    "# get the parameters of Gaussian Naive Bayes\n",
    "print(Gaussian_NB.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'var_smoothing': 1e-09}\n",
      "Best cross-validation score: 0.942\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# define the hyperparameters and their values\n",
    "param_grid = {\n",
    "    'var_smoothing': [1e-9, 1e-10, 1e-11, 1e-12],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(GaussianNB(), param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f'Best parameters: {grid_search.best_params_}')\n",
    "print(f'Best cross-validation score: {grid_search.best_score_:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Models and Test Set Scores:\n",
      "Decision Tree: 1.000\n",
      "Random Forest: 1.000\n",
      "Logistic Regression: 1.000\n",
      "KNN: 1.000\n",
      "SVM: 1.000\n",
      "Naive Bayes: 1.000\n",
      "GradientBoostingClassifier: 1.000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the models and their parameter grids\n",
    "models = {\n",
    "    'Decision Tree': (DecisionTreeClassifier(), {'max_depth': [2, 4, 6, 8, 10]}),\n",
    "    'Random Forest': (RandomForestClassifier(), {'n_estimators': [10, 50, 100, 200]}),\n",
    "    'Logistic Regression': (LogisticRegression(), {'C': [0.1, 1, 10, 100]}),\n",
    "    'KNN': (KNeighborsClassifier(), {'n_neighbors': [3, 5, 7, 9, 11]}),\n",
    "    'SVM': (SVC(), {'C': [0.1, 1, 10, 100], 'kernel': ['linear', 'rbf']}),\n",
    "    'Naive Bayes': (GaussianNB(), {'var_smoothing': [1e-9, 1e-10, 1e-11, 1e-12]}),\n",
    "    'GradientBoostingClassifier': (GradientBoostingClassifier(), {'n_estimators': [100, 200, 300, 400], 'max_depth': [3, 5, 7, 9]})\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV for each model\n",
    "best_models = {}\n",
    "for name, (model, param_grid) in models.items():\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_models[name] = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best models on the test set\n",
    "results = {}\n",
    "for name, model in best_models.items():\n",
    "    test_score = model.score(X_test, y_test)\n",
    "    results[name] = test_score\n",
    "\n",
    "# Print the best models and their test set scores\n",
    "print('Best Models and Test Set Scores:')\n",
    "for name, score in results.items():\n",
    "    print(f'{name}: {score:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Models\n",
    "#### 1. Logistic Regression\n",
    "#### 2. K-Nearest Neighbors\n",
    "#### 3. Support Vector Machines\n",
    "#### 4. Decision Trees\n",
    "#### 5. Random Forest\n",
    "#### 6. Gradient Boosting\n",
    "#### 7. XGBoost\n",
    "#### 8. LightGBM\n",
    "#### 9. CatBoost\n",
    "#### 10. Neural Networks\n",
    "#### 11. Naive Bayes\n",
    "#### 12. Linear Discriminant Analysis\n",
    "#### 13. Quadratic Discriminant Analysis\n",
    "#### 14. AdaBoost\n",
    "#### 15. Bagging\n",
    "#### 16. Extra Trees\n",
    "#### 17. Stochastic Gradient Descent\n",
    "#### 18. Perceptron\n",
    "#### 19. Passive Aggressive Classifier\n",
    "#### 20. Ridge Classifier\n",
    "#### 21. Nearest Centroid\n",
    "#### 22. Gaussian Process\n",
    "#### 23. MLP Classifier\n",
    "#### 24. RBF SVM\n",
    "#### 25. Linear SVM\n",
    "#### 26. Nu-SVM\n",
    "#### 27. One-Class SVM\n",
    "#### 28. Isolation Forest\n",
    "#### 29. Local Outlier Factor\n",
    "#### 30. Elliptic Envelope\n",
    "#### 31. One-Class SVM\n",
    "#### 32. Local Outlier Factor\n",
    "#### 33. Elliptic Envelope\n",
    "#### 34. DBSCAN\n",
    "#### 35. Birch\n",
    "#### 36. Spectral Clustering\n",
    "#### 37. Affinity Propagation\n",
    "#### 38. Mean Shift\n",
    "#### 39. Agglomerative Clustering\n",
    "#### 40. K-Means\n",
    "#### 41. Mini-Batch K-Means\n",
    "#### 42. Gaussian Mixture Model\n",
    "#### 43. Principal Component Analysis\n",
    "#### 44. Kernel PCA\n",
    "#### 45. Locally Linear Embedding\n",
    "#### 46. MDS\n",
    "#### 47. Isomap\n",
    "#### 48. Spectral Embedding\n",
    "#### 49. TSNE\n",
    "#### 50. UMAP\n",
    "#### 51. Autoencoder\n",
    "#### 52. Variational Autoencoder\n",
    "#### 53. Generative Adversarial Network\n",
    "#### 54. Deep Belief Network\n",
    "#### 55. Restricted Boltzmann Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Models\n",
    "#### 1. Linear Regression\n",
    "#### 2. Lasso\n",
    "#### 3. Ridge\n",
    "#### 4. Elastic Net\n",
    "#### 5. Polynomial Regression\n",
    "#### 6. Stochastic Gradient Descent\n",
    "#### 7. K-Nearest Neighbors\n",
    "#### 8. Decision Trees\n",
    "#### 9. Random Forest\n",
    "#### 10. Gradient Boosting\n",
    "#### 11. XGBoost\n",
    "#### 12. LightGBM\n",
    "#### 13. CatBoost\n",
    "#### 14. Neural Networks\n",
    "#### 15. Support Vector Machines\n",
    "#### 16. Gaussian Process\n",
    "#### 17. AdaBoost\n",
    "#### 18. Bagging\n",
    "#### 19. Extra Trees\n",
    "#### 20. Ridge Regression\n",
    "#### 21. Bayesian Ridge\n",
    "#### 22. ARD Regression\n",
    "#### 23. Huber Regression\n",
    "#### 24. RANSAC Regression\n",
    "#### 25. TheilSen Regression\n",
    "#### 26. Poisson Regression\n",
    "#### 27. Tweedie Regression\n",
    "#### 28. Gamma Regression\n",
    "#### 29. Inverse Gaussian Regression\n",
    "#### 30. Orthogonal Matching Pursuit\n",
    "#### 31. Bayesian Ridge\n",
    "#### 32. Automatic Relevance Determination\n",
    "#### 33. Least Angle Regression\n",
    "#### 34. LARS Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'fit_intercept': True}\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "df = sns.load_dataset('titanic')\n",
    "df['age'] = df['age'].fillna(df['age'].mean())\n",
    "X = df[['age']]\n",
    "y = df['fare']\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# create the model\n",
    "model = LinearRegression()\n",
    "\n",
    "# fit the model\n",
    "model.fit(X, y)\n",
    "\n",
    "# define the hyperparameters and their values\n",
    "param_grid = {'fit_intercept': [True, False]}\n",
    "\n",
    "# object grid search cv (Creating the model)\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='r2')\n",
    "\n",
    "# fit and train the model\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# print the best parameters and the best score\n",
    "print(f'Best parameters: {grid_search.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'n_neighbors': 13, 'p': 1, 'weights': 'distance'}\n",
      "Best Score: 0.6783777334912072\n"
     ]
    }
   ],
   "source": [
    "# KNN\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "df = sns.load_dataset('titanic')\n",
    "X = df[['age', 'sex', 'pclass', 'sibsp', 'parch', 'fare',]]\n",
    "y = df['survived']\n",
    "X = pd.get_dummies(X, columns=['sex'])\n",
    "X.age.fillna(value=X['age'].mean(), inplace=True)\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# create the model\n",
    "model = KNeighborsClassifier()\n",
    "\n",
    "# fit the model\n",
    "model.fit(X, y)\n",
    "\n",
    "# define the hyperparameters and their values\n",
    "param_grid = {'n_neighbors': np.arange(1,20,2), 'weights': ['uniform', 'distance'], 'p': [1, 2, 3]}\n",
    "\n",
    "# object grid search cv (Creating the model)\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='f1', n_jobs=-1)\n",
    "\n",
    "# fit and train the model\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# print the best parameters and the best score\n",
    "print(f'Best parameters: {grid_search.best_params_}')\n",
    "print(f'Best Score: {grid_search.best_score_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scoring of the KNN model\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# For classification\n",
    "model = LogisticRegression()\n",
    "scores = cross_val_score(model, X, y, scoring='accuracy')  # Uses accuracy score\n",
    "scores = cross_val_score(model, X, y, scoring='f1')  # Uses F1-score\n",
    "scores = cross_val_score(model, X, y, scoring='roc_auc')  # Uses AUC-ROC\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# For regression\n",
    "model = LinearRegression()\n",
    "scores = cross_val_score(model, X, y, scoring='neg_mean_squared_error')  # Uses negative MSE\n",
    "scores = cross_val_score(model, X, y, scoring='r2')  # Uses R-squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'max_depth': 6, 'min_samples_split': 8}\n",
      "Best Score: 0.7447162294133822\n"
     ]
    }
   ],
   "source": [
    "# DTC\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "df = sns.load_dataset('titanic')\n",
    "X = df[['age', 'sex', 'pclass', 'sibsp', 'parch', 'fare',]]\n",
    "y = df['survived']\n",
    "X = pd.get_dummies(X, columns=['sex'])\n",
    "X.age.fillna(value=X['age'].mean(), inplace=True)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# create the model\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "# fit the model\n",
    "model.fit(X, y)\n",
    "\n",
    "# define the hyperparameters and their values\n",
    "param_grid = {'max_depth': np.arange(1, 10), 'min_samples_split': np.arange(2, 10)}\n",
    "\n",
    "# object grid search cv (Creating the model)\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, n_jobs=-1, scoring='f1')\n",
    "\n",
    "# fit and train the model\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# print the best parameters and the best score\n",
    "print(f'Best parameters: {grid_search.best_params_}')\n",
    "print(f'Best Score: {grid_search.best_score_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'max_depth': 9, 'n_estimators': 300}\n",
      "Best Score: 0.7667802426824764\n"
     ]
    }
   ],
   "source": [
    "# RF\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "df = sns.load_dataset('titanic')\n",
    "X = df[['age', 'sex', 'pclass', 'sibsp', 'parch', 'fare',]]\n",
    "y = df['survived']\n",
    "X = pd.get_dummies(X, columns=['sex'])\n",
    "X.age.fillna(value=X['age'].mean(), inplace=True)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# create the model\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# fit the model\n",
    "model.fit(X, y)\n",
    "\n",
    "# define the hyperparameters and their values\n",
    "param_grid = {'n_estimators': [100, 200, 300, 400], 'max_depth': [3, 5, 7, 9]}\n",
    "\n",
    "# object grid search cv (Creating the model)\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, n_jobs=-1, scoring='f1')\n",
    "\n",
    "# fit and train the model\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# print the best parameters and the best score\n",
    "print(f'Best parameters: {grid_search.best_params_}')\n",
    "print(f'Best Score: {grid_search.best_score_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.1, 'loss': 'log_loss', 'max_depth': 3, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_iter_no_change': None, 'random_state': None, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gbc = GradientBoostingClassifier()\n",
    "\n",
    "# get the parameters of Gradient Boosting\n",
    "print(gbc.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'learning_rate': 0.1, 'max_depth': 3, 'min_samples_split': 5, 'n_estimators': 200, 'subsample': 0.8, 'warm_start': False}\n",
      "Best Score: 0.7763292161970624\n"
     ]
    }
   ],
   "source": [
    "# GBC\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "df = sns.load_dataset('titanic')\n",
    "X = df[['age', 'sex', 'pclass', 'sibsp', 'parch', 'fare',]]\n",
    "y = df['survived']\n",
    "X = pd.get_dummies(X, columns=['sex'])\n",
    "X.age.fillna(value=X['age'].mean(), inplace=True)\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# create the model\n",
    "model = GradientBoostingClassifier()\n",
    "\n",
    "# fit the model\n",
    "model.fit(X, y)\n",
    "\n",
    "# define the hyperparameters and their values\n",
    "param_grid = {\n",
    "    'n_estimators': [200],\n",
    "    'learning_rate': [0.1],\n",
    "    'max_depth': [3],\n",
    "    'min_samples_split': [5],\n",
    "    'subsample': [0.8],\n",
    "    'warm_start': [False]\n",
    "}\n",
    "\n",
    "# object grid search cv (Creating the model)\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, n_jobs=-1, scoring='f1')\n",
    "\n",
    "# fit and train the model\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# print the best parameters and the best score\n",
    "print(f'Best parameters: {grid_search.best_params_}')\n",
    "print(f'Best Score: {grid_search.best_score_}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
